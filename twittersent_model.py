# -*- coding: utf-8 -*-
"""TwitterSent Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/196DldDRkkKMvww-ZeALlRvxeUP9_jJoZ

#**Twitter Sentiment Analysis Dataset Model Training**
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

path = ('/content/drive/My Drive/Copy of Sentiment Analysis Dataset.csv')
path2 =('/content/drive/My Drive/Colab Notebooks/Copy of Copy of trump_merged.pkl')

"""# Import Training Dataset"""

import csv
tweets = []
sent_label = []
with open (path, encoding='utf8') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    next(reader)
    for row in reader:
        sent_label.append(row[1])
        tweets.append(row[3])

columns = ['sent_label','tweets']
dataset = pd.DataFrame({'tweets':tweets,'sent_label':sent_label})
dataset.info()

"""Split dengan rasio 80-20 untuk training"""

train_dataset = dataset.tweets[:1262890]
test_dataset = dataset.tweets[1262890:]

train_labels = dataset.sent_label[:1262890]
test_labels = dataset.sent_label[1262890:]

train_dataset

test_labels

"""# Pre-processing Training Dataset

Training dataset di normalisasi agar text (corpus) lebih seragam
"""

import string
import re
import os
import nltk
nltk.download('twitter_samples')
nltk.download('stopwords')
from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords, twitter_samples 

tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)

stopwords_english = stopwords.words('english')

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

def process_tweet(tweet):

    tweet = re.sub(r'\$\w*', '', tweet)
    tweet = re.sub(r'^RT[\s]+', '', tweet)
    tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)    
    tweet = re.sub(r'#', '', tweet)
    
    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)
    tweet_tokens = tokenizer.tokenize(tweet)
    
    tweets_clean = []
    for word in tweet_tokens:
        if (word not in stopwords_english and # remove stopwords
            word not in string.punctuation): # remove punctuation
            #stem_word = stemmer.stem(word) # stemming word
            tweets_clean.append(word)
    
    return tweets_clean

"""Terdapat sebanyak 276226 unique words dari training dataset yang sudah dibersihkan"""

Vocab = {}
for tweet in train_dataset: 
    processed_tweet = process_tweet(tweet)
    for word in processed_tweet:
        if word not in Vocab: 
            Vocab[word] = len(Vocab)

print(len(Vocab))

"""Tokenize training dataset menjadi vector"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size = 276226
embedding_dim = 16
max_length = 140
trunc_type='post'
oov_tok = "<OOV>"

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok )
tokenizer.fit_on_texts(train_dataset)
dictionary = tokenizer.word_index
train_sequences = tokenizer.texts_to_sequences(train_dataset)
train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')

test_sequences = tokenizer.texts_to_sequences(test_dataset)
test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')

train_lab = pd.get_dummies(train_labels).values
test_lab = pd.get_dummies(test_labels).values
test_lab

"""# Model Development"""

model = tf.keras.Sequential([
    
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),
    tf.keras.layers.SpatialDropout1D(0.3),
    tf.keras.layers.GaussianNoise(0.2),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.25, 
                                                       recurrent_dropout=0.2,
                                                       return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(2, activation='sigmoid',kernel_regularizer=tf.keras.regularizers.l2(0.0001))
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

import numpy as np

new_model = tf.keras.models.load_model("/content/drive/My Drive/TA2_gitsent_model_checkpoint.hdf5")
#Training sebelumnya terhenti pada iterasi ke 4 karena disconnected dari Google Colab, tetapi model sudah di save setiap checkpoint(iterasi) 
#Maka dilakukan tf.keras.models.load_model untuk load dari checkpoint 4 dan diiterasi ulang sebanyak 4 kali sehingga iterasi menjadi total 8 iterasi

checkpoint = tf.keras.callbacks.ModelCheckpoint('/content/drive/My Drive/TA2_aftercheckpoint_gitsent_model_checkpoint.hdf5',
                                                monitor='loss', verbose=1, save_best_only=True, mode='auto')

epoch = 4 #4 iterasi tambahan
batch =256
history = new_model.fit(train_padded, train_lab, epochs=epoch, batch_size=batch,
                    verbose=1,
                    validation_data=(test_padded, test_lab), 
                    shuffle=True, callbacks=[checkpoint])
#history = model.fit(train_padded, train_labels, epochs=epoch, validation_data=(test_padded, test_labels), verbose=1, shuffle=True)

import matplotlib.pyplot as plt # Impot the relevant module

fig, ax = plt.subplots() # Create the figure and axes object
ax = hasil.plot('epoch','acc')
ax1 = ax.twinx()
hasil.plot('epoch','val_acc',ax=ax1, color='orange')

import matplotlib.pyplot as plt # Impot the relevant module


plt.plot(hasil['loss'])
plt.plot(hasil['val_loss'])
plt.xlabel("Epochs")
plt.ylabel('accuracy')
plt.legend(['accuracy', 'val_accuracy'])
plt.show()



import matplotlib.pyplot as plt 
#dikarenakan setelah  4 iterasi tambahan koneksi ke Google Colab terputus kembali maka acc-val_acc dengan loss-vall_loss tidak dapat diplot

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_modeval(loaded_model,"loss")
plot_modeval(loaded_model,"accuracy")

"""# Model Prediction"""

loaded_model = tf.keras.models.load_model("/content/drive/My Drive/TA2_aftercheckpoint_gitsent_model_checkpoint.hdf5")#load dari checkpoint yang baru, abaikan jika colab tidak disconnected

trump_dir = '/content/drive/My Drive/TA2/data/predict_data/trump_dataset/Trump-november2020-11-03.csv'
biden_dir = '/content/drive/My Drive/TA2/data/predict_data/biden_dataset/Biden-november2020-11-03.csv'

trump_df = pd.read_csv(trump_dir, encoding='utf8', delimiter=',')
biden_df = pd.read_csv(biden_dir, encoding='utf8', delimiter=',')

trump_df = trump_df.drop_duplicates(subset=["username"],keep=False)

trump_df

biden_df = biden_df.drop_duplicates(subset=["username"],keep=False)

biden_df

2

trump_tweet = trump_df['tweet']
biden_tweet = biden_df['tweet']

def clean_tweet(tweet):

    tweet = re.sub(r'\$\w*', '', tweet)
    tweet = re.sub(r'^RT[\s]+', '', tweet)
    tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)    
    tweet = re.sub(r'#', '', tweet)
    
    return(tweet)

cleaned_trump = []
cleaned_biden = []

for tweet in trump_tweet: 
    cleaned_trump.append(clean_tweet(tweet))

for tweet in biden_tweet:
    cleaned_biden.append(clean_tweet(tweet))

tkz_trump = tokenizer.texts_to_sequences(cleaned_trump)
tkz_trump_padded = pad_sequences(tkz_trump, maxlen=max_length, padding='post', truncating='post')

tkz_biden = tokenizer.texts_to_sequences(cleaned_biden)
tkz_biden_padded = pad_sequences(tkz_biden, maxlen=max_length, padding='post', truncating='post')

prediction_trump = loaded_model.predict(tkz_trump_padded)
prediction_biden = loaded_model.predict(tkz_biden_padded)

for i in prediction_trump:
  print(i)

polar_trump = []
polar_biden = []

for i in range(len(prediction_trump)):
    if prediction_trump[i][0] > 0.5:
        polar_trump.append('Negative')
    else:
        polar_trump.append('Positive')

for i in range(len(prediction_biden)):
    if prediction_biden[i][0] > 0.5:
        polar_biden.append('Negative')
    else:
        polar_biden.append('Positive')

tr_sentiment = pd.DataFrame(trump_df['tweet'])
tr_sentiment['polarity'] = polar_trump

bd_sentiment = pd.DataFrame(biden_df['tweet'])
bd_sentiment['polarity'] = polar_biden

tr_sentiment

bd_sentiment

fig, ax = plt.subplots(figsize=(4, 5))
ax.hist(tr_sentiment.polarity, bins=2)
for rect in ax.patches:
    height = rect.get_height()
    ax.annotate(f'{int(height)}', xy=(rect.get_x()+rect.get_width()/2, height), 
          xytext=(0, 5), textcoords='offset points', ha='center', va='bottom')
        
plt.title("Donald Trump Sentiment")
plt.show()

fig, ax = plt.subplots(figsize=(4, 5))
ax.hist(bd_sentiment.polarity, bins=2)
for rect in ax.patches:
    height = rect.get_height()
    ax.annotate(f'{int(height)}', xy=(rect.get_x()+rect.get_width()/2, height), 
          xytext=(0, 5), textcoords='offset points', ha='center', va='bottom')
        
plt.title("Joe Biden Sentiment")
plt.show()

"""# Word Cloud"""

tweet_trump = pd.DataFrame(tr_sentiment['tweet'])
tweet_biden = pd.DataFrame(bd_sentiment['tweet'])

def to_lower(tweet):
  tweet = tweet.lower()
  tweet =re.sub('\n','', tweet)

  return tweet

tweet_trump['tweet'] = tweet_trump.tweet.apply(lambda x: to_lower(x))
tweet_biden['tweet'] = tweet_biden.tweet.apply(lambda x: to_lower(x))

stop_words = nltk.corpus.stopwords.words('english')
new_words = ["able","about","above","abroad","according","accordingly","across","actually","adj",
             "after","afterwards","again","against","ago","ahead","ain’t","all","allow","allows",
             "almost","alone","along","alongside","already","also","although","always","am","amid",
             "amidst","among","amongst","an","and","another","any","anybody","anyhow","anyone","anything",
             "anyway","anyways","anywhere","apart","appear","appreciate","appropriate","are","aren’t","around",
             "as","a’s","aside","ask","asking","associated","at","available","away","awfully","back","backward",
             "backwards","be","became","because","become","becomes","becoming","been","before","beforehand",
             "begin","behind","being","believe","below","beside","besides","best","better","between","beyond",
             "both","brief","but","by","came","can","cannot","cant","can’t","caption","cause","causes","certain",
             "certainly","changes","clearly","c’mon","co","co.","com","come","comes","concerning","consequently",
             "consider","considering","contain","containing","contains","corresponding","could","couldn’t","course",
             "c’s","currently","dare","daren’t","definitely","described","despite","did","didn’t","different","directly",
             "do","does","doesn’t","doing","done","don’t","down","downwards","during","each","edu","eg","eight","eighty",
             "either","else","elsewhere","end","ending","enough","entirely","especially","et","etc","even","ever","evermore",
             "every","everybody","everyone","everything","everywhere","ex","exactly","example","except","fairly","far",
             "farther","few","fewer","fifth","first","five","followed","following","follows","for","forever","former",
             "formerly","forth","forward","found","four","from","further","furthermore","get","gets","getting","given","gives",
             "go","goes","going","gone","got","gotten","greetings","had","hadn’t","half","happens","hardly","has","hasn’t","have",
             "haven’t","having","he","he’d","he’ll","hello","help","hence","her","here","hereafter","hereby","herein","here’s",
             "hereupon","hers","herself","he’s","hi","him","himself","his","hither","hopefully","how","howbeit","however",
             "hundred","i’d","ie","if","ignored","i’ll","i’m","immediate","in","inasmuch","inc","inc.","indeed","indicate",
             "indicated","indicates","inner","inside","insofar","instead","into","inward","is","isn’t","it","it’d","it’ll",
             "its","it’s","itself","i’ve","just","k","keep","keeps","kept","know","known","knows","last","lately","later",
             "latter","latterly","least","less","lest","let","let’s","like","liked","likely","likewise","little","look",
             "looking","looks","low","lower","ltd","made","mainly","make","makes","many","may","maybe","mayn’t","me",
             "mean","meantime","meanwhile","merely","might","mightn’t","mine","minus","miss","more","moreover","most",
             "mostly","mr","mrs","much","must","mustn’t","my","myself","name","namely","nd","near","nearly","necessary",
             "need","needn’t","needs","neither","never","neverf","neverless","nevertheless","new","next","nine","ninety",
             "no","nobody","non","none","nonetheless","noone","no-one","nor","normally","not","nothing","notwithstanding",
             "novel","now","nowhere","obviously","of","off","often","oh","ok","okay","old","on","once","one","ones","one’s",
             "only","onto","opposite","or","other","others","otherwise","ought","oughtn’t","our","ours","ourselves","out",
             "outside","over","overall","own","particular","particularly","past","per","perhaps","placed","please","plus",
             "possible","presumably","probably","provided","provides","que","quite","qv","rather","rd","re","really","reasonably",
             "recent","recently","regarding","regardless","regards","relatively","respectively","right","round","said","same",
             "saw","say","saying","says","second","secondly","see","seeing","seem","seemed","seeming","seems","seen","self",
             "selves","sensible","sent","serious","seriously","seven","several","shall","shan’t","she","she’d","she’ll","she’s",
             "should","shouldn’t","since","six","so","some","somebody","someday","somehow","someone","something","sometime",
             "sometimes","somewhat","somewhere","soon","sorry","specified","specify","specifying","still","sub","such","sup",
             "sure","take","taken","taking","tell","tends","th","than","thank","thanks","thanx","that","that’ll","thats","that’s",
             "that’ve","the","their","theirs","them","themselves","then","thence","there","thereafter","thereby","there’d",
             "therefore","therein","there’ll","there’re","theres","there’s","thereupon","there’ve","these","they","they’d",
             "they’ll","they’re","they’ve","thing","things","think","third","thirty","this","thorough","thoroughly","those",
             "though","three","through","throughout","thru","thus","till","to","together","too","took","toward","towards",
             "tried","tries","truly","try","trying","t’s","twice","two","un","under","underneath","undoing","unfortunately",
             "unless","unlike","unlikely","until","unto","up","upon","upwards","us","use","used","useful","uses","using",
             "usually","v","value","various","versus","very","via","viz","vs","want","wants","was","wasn’t","way","we",
             "we’d","welcome","well","we’ll","went","were","we’re","weren’t","we’ve","what","whatever","what’ll","what’s",
             "what’ve","when","whence","whenever","where","whereafter","whereas","whereby","wherein","where’s","whereupon",
             "wherever","whether","which","whichever","while","whilst","whither","who","who’d","whoever","whole","who’ll",
             "whom","whomever","who’s","whose","why","will","willing","wish","with","within","without","wonder","won’t",
             "would","wouldn’t","yes","yet","you","you’d","you’ll","your","you’re","yours","yourself","yourselves","you’ve",
             "zero","a","how’s","i","when’s","why’s","b","c","d","e","f","g","h","j","l","m","n","o","p","q","r","s","t","u",
             "uucp","w","x","y","z","I","www","amount","bill","bottom","call","computer","con","couldnt","cry","de","describe",
             "detail","due","eleven","empty","fifteen","fifty","fill","find","fire","forty","front","full","give","hasnt",
             "herse","himse","interest","itse”","mill","move","myse”","part","put","show","side","sincere","sixty","system",
             "ten","thick","thin","top","twelve","twenty","abst","accordance","act","added","adopted","affected","affecting",
             "affects","ah","announce","anymore","apparently","approximately","aren","arent","arise","auth","beginning",
             "beginnings","begins","biol","briefly","ca","date","ed","effect","et-al","ff","fix","gave","giving","heres",
             "hes","hid","home","id","im","immediately","importance","important","index","information","invention","itd",
             "keys","kg","km","largely","lets","line","‘ll","means","mg","million","ml","mug","na","nay","necessarily",
             "nos","noted","obtain","obtained","omitted","ord","owing","page","pages","poorly","possibly","potentially",
             "pp","predominantly","present","previously","primarily","promptly","proud","quickly","ran","readily","ref",
             "refs","related","research","resulted","resulting","results","run","sec","section","shed","shes","showed",
             "shown","showns","shows","significant","significantly","similar","similarly","slightly","somethan",
             "specifically","state","states","stop","strongly","substantially","successfully","sufficiently","suggest",
             "thered","thereof","therere","thereto","theyd","theyre","thou","thoughh","thousand","throug","til","tip",
             "ts","ups","usefully","usefulness","‘ve","vol","vols","wed","whats","wheres","whim","whod","whos","widely",
             "words","world","youd","youre","donald","trump","donald trump","amp","biden","joe","trumpdonald", "trump donald", 
             "bidenjoe", "biden joe", "president","people"]
stop_words.extend(new_words)
stop_words = set(stop_words)

tweet_trump['tweet'] = tweet_trump.tweet.apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))
tweet_biden['tweet'] = tweet_biden.tweet.apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))

tweet_trump

from wordcloud import WordCloud
text = " ".join(words for words in tweet_trump.tweet)
wordcloud = WordCloud(stopwords=stop_words, width=1600, height=900, background_color="white", 
                       colormap="Dark2", max_font_size=700, random_state=5, max_words=10, collocations=False).generate(text)

plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

from wordcloud import WordCloud
text = " ".join(words for words in tweet_biden.tweet)
wordcloud = WordCloud(stopwords=stop_words, width=1600, height=900, background_color="white", 
                       colormap="Dark2", max_font_size=700, random_state=5, max_words=10, collocations=False).generate(text)

plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()













checkpoint = tf.keras.callbacks.ModelCheckpoint("/content/drive/My Drive/TA2_gitsent_model_checkpoint.hdf5", 
                                                monitor='loss', verbose=1,
                                                save_best_only=True, mode='auto')
epoch = 8
batch = 64
history = model.fit(train_padded, train_lab, epochs=epoch, batch_size=batch,
                    verbose=1,
                    validation_data=(test_padded, test_lab), 
                    shuffle=True, callbacks=[checkpoint])
#history = model.fit(train_padded, train_labels, epochs=epoch, validation_data=(test_padded, test_labels), verbose=1, shuffle=True)

EDA_trump = t

new_model = tf.keras.models.load_model('/content/drive/My Drive/TA2_gitsent_model_checkpoint.hdf5')
np.testing.assert_allclose(model.predict(train_padded), new_model.predict(train_padded),1e-5)

checkpoint = tf.keras.callbacks.ModelCheckpoint('/content/drive/My Drive/TA2_aftercheckpoint_gitsent_model_checkpoint.hdf5',
                                                monitor='loss', verbose=1, save_best_only=True, mode='auto')
#callbacks_list = [checkpoint]
new_model.fit(train_padded, train_lab, epochs=epoch, batch_size=batch,
                    verbose=2,
                    validation_data=(test_padded, test_lab), 
                    shuffle=True, callbacks=[checkpoint])

model.save('/content/drive/My Drive/Colab Notebooks/gitsent_model.h5')